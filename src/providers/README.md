# OpenChat Providers

Modular provider system for connecting to different LLM backends.

## ğŸ“ Directory Structure

```
src/providers/
â”œâ”€â”€ ollama/              # Ollama provider
â”‚   â”œâ”€â”€ index.ts
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ lmstudio/           # LM Studio provider
â”‚   â”œâ”€â”€ index.ts
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ llamacpp/           # llama.cpp provider
â”‚   â”œâ”€â”€ index.ts
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ anthropic/          # Anthropic Claude provider
â”‚   â”œâ”€â”€ index.ts
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ openai/             # OpenAI GPT provider
â”‚   â”œâ”€â”€ index.ts
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ base.ts             # Base provider class
â”œâ”€â”€ factory.ts          # Provider factory
â””â”€â”€ index.ts            # Main exports
```

## ğŸš€ Available Providers

### Local Providers

#### Ollama
**Default**: `http://localhost:11434`  
Local LLM runtime with easy model management.

[Documentation](./ollama/README.md)

#### LM Studio
**Default**: `http://localhost:1234`  
Desktop application for running quantized models.

[Documentation](./lmstudio/README.md)

#### llama.cpp
**Default**: `http://localhost:8080`  
High-performance inference server for GGUF models.

[Documentation](./llamacpp/README.md)

### Cloud Providers

#### Anthropic
**Default**: `https://api.anthropic.com`  
Claude models with advanced reasoning capabilities. **Requires API Key**.

[Documentation](./anthropic/README.md)

#### OpenAI
**Default**: `https://api.openai.com/v1`  
GPT models including GPT-4o and o1. **Requires API Key**.

[Documentation](./openai/README.md)

## ğŸ”§ Adding a New Provider

### 1. Create Provider Directory

```
src/providers/your-provider/
â”œâ”€â”€ index.ts       # Provider implementation
â””â”€â”€ README.md      # Documentation
```

### 2. Implement Provider Class

```typescript
// src/providers/your-provider/index.ts
import { BaseProvider } from '../base'
import type { ChatCompletionRequest, ModelInfo } from '../../types'

export class YourProvider extends BaseProvider {
  async listModels(): Promise<ModelInfo[]> {
    // Implement model listing
  }

  async sendMessage(
    request: ChatCompletionRequest,
    onChunk?: (content: string) => void
  ): Promise<string> {
    // Implement message sending
  }

  async testConnection(): Promise<boolean> {
    // Implement connection test
  }
}
```

### 3. Register in Factory

```typescript
// src/providers/factory.ts
import { YourProvider } from './your-provider'

export class ProviderFactory {
  static createProvider(config: ProviderConfig): BaseProvider {
    switch (config.type) {
      // ... existing cases
      case 'your-provider':
        return new YourProvider(config)
      default:
        throw new Error(`Unsupported provider type: ${config.type}`)
    }
  }
}
```

### 4. Add Type Definition

```typescript
// src/types/index.ts
export type ProviderType = 
  | 'ollama' 
  | 'lmstudio' 
  | 'llamacpp'
  | 'your-provider' // Add here
```

### 5. Add Default Config

```typescript
// src/providers/factory.ts
static getDefaultConfig(type: ProviderType): ProviderConfig {
  const defaults: Record<ProviderType, ProviderConfig> = {
    // ... existing configs
    'your-provider': {
      type: 'your-provider',
      name: 'Your Provider',
      baseUrl: 'http://localhost:PORT',
      enabled: true,
    },
  }
  return defaults[type]
}
```

## ğŸ“š Base Provider Class

All providers extend `BaseProvider` which provides:

- **fetchWithTimeout**: HTTP requests with timeout
- **config**: Provider configuration
- **Abstract methods**: `listModels()`, `sendMessage()`, `testConnection()`

## ğŸ¯ Best Practices

### âœ… DO
- Extend `BaseProvider`
- Implement all abstract methods
- Handle errors gracefully
- Support streaming when possible
- Test connection before use
- Document your provider

### âŒ DON'T
- Hardcode URLs or ports
- Ignore timeout settings
- Skip error handling
- Break the interface contract

## ğŸ” Provider Interface

```typescript
interface Provider {
  // List available models
  listModels(): Promise<ModelInfo[]>
  
  // Send message (streaming or non-streaming)
  sendMessage(
    request: ChatCompletionRequest,
    onChunk?: (content: string) => void
  ): Promise<string>
  
  // Test if provider is reachable
  testConnection(): Promise<boolean>
}
```

## ğŸ“– Examples

Check existing providers for reference:
- **Simple**: `ollama/` - Custom API format
- **OpenAI-compatible**: `lmstudio/`, `llamacpp/` - Standard format
- **Streaming**: All providers support streaming

## ğŸ¤ Contributing

1. Create your provider directory
2. Implement the provider class
3. Add documentation
4. Test thoroughly
5. Submit a Pull Request

---

**Happy Provider Development! ğŸš€**
